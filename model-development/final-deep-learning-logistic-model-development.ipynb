{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 263ms/step - accuracy: 0.6966 - loss: 0.6243 - val_accuracy: 0.6800 - val_loss: 0.6387 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.7071 - loss: 0.6082 - val_accuracy: 0.7000 - val_loss: 0.6131 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.7144 - loss: 0.5910 - val_accuracy: 0.6800 - val_loss: 0.6039 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.7167 - loss: 0.5712 - val_accuracy: 0.6867 - val_loss: 0.5960 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.7297 - loss: 0.5553 - val_accuracy: 0.6933 - val_loss: 0.5841 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.7276 - loss: 0.5510 - val_accuracy: 0.6867 - val_loss: 0.5689 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.7264 - loss: 0.5373 - val_accuracy: 0.7133 - val_loss: 0.5553 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.7506 - loss: 0.5200 - val_accuracy: 0.7200 - val_loss: 0.5399 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.7409 - loss: 0.5125 - val_accuracy: 0.7267 - val_loss: 0.5260 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.7426 - loss: 0.4957 - val_accuracy: 0.7200 - val_loss: 0.5106 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.7448 - loss: 0.4897 - val_accuracy: 0.7200 - val_loss: 0.4988 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.7732 - loss: 0.4848 - val_accuracy: 0.7200 - val_loss: 0.4915 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.7645 - loss: 0.4824 - val_accuracy: 0.7400 - val_loss: 0.4858 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 97ms/step - accuracy: 0.7747 - loss: 0.4643 - val_accuracy: 0.7467 - val_loss: 0.4800 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.7582 - loss: 0.4539 - val_accuracy: 0.7533 - val_loss: 0.4738 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.7696 - loss: 0.4577 - val_accuracy: 0.7667 - val_loss: 0.4683 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.7817 - loss: 0.4450 - val_accuracy: 0.7800 - val_loss: 0.4635 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.7887 - loss: 0.4303 - val_accuracy: 0.7867 - val_loss: 0.4633 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.7856 - loss: 0.4480 - val_accuracy: 0.8000 - val_loss: 0.4580 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.7985 - loss: 0.4245 - val_accuracy: 0.8067 - val_loss: 0.4506 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.7951 - loss: 0.4366 - val_accuracy: 0.8133 - val_loss: 0.4468 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.7996 - loss: 0.4288 - val_accuracy: 0.8200 - val_loss: 0.4431 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.7914 - loss: 0.4272 - val_accuracy: 0.8200 - val_loss: 0.4375 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.7987 - loss: 0.4266 - val_accuracy: 0.8200 - val_loss: 0.4306 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.8160 - loss: 0.4140 - val_accuracy: 0.8267 - val_loss: 0.4292 - learning_rate: 0.0010\n",
      "Epoch 26/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.8073 - loss: 0.4108 - val_accuracy: 0.8133 - val_loss: 0.4203 - learning_rate: 0.0010\n",
      "Epoch 27/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.8079 - loss: 0.3976 - val_accuracy: 0.8200 - val_loss: 0.4130 - learning_rate: 0.0010\n",
      "Epoch 28/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.8163 - loss: 0.3970 - val_accuracy: 0.8133 - val_loss: 0.4089 - learning_rate: 0.0010\n",
      "Epoch 29/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.8084 - loss: 0.3973 - val_accuracy: 0.8267 - val_loss: 0.4054 - learning_rate: 0.0010\n",
      "Epoch 30/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.8357 - loss: 0.3594 - val_accuracy: 0.8267 - val_loss: 0.4018 - learning_rate: 0.0010\n",
      "Epoch 31/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.8315 - loss: 0.3723 - val_accuracy: 0.8333 - val_loss: 0.3961 - learning_rate: 0.0010\n",
      "Epoch 32/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.8267 - loss: 0.3752 - val_accuracy: 0.8400 - val_loss: 0.3910 - learning_rate: 0.0010\n",
      "Epoch 33/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.8421 - loss: 0.3519 - val_accuracy: 0.8467 - val_loss: 0.3865 - learning_rate: 0.0010\n",
      "Epoch 34/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8571 - loss: 0.3475 - val_accuracy: 0.8267 - val_loss: 0.3876 - learning_rate: 0.0010\n",
      "Epoch 35/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.8534 - loss: 0.3381 - val_accuracy: 0.8400 - val_loss: 0.3789 - learning_rate: 0.0010\n",
      "Epoch 36/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.8445 - loss: 0.3456 - val_accuracy: 0.8467 - val_loss: 0.3725 - learning_rate: 0.0010\n",
      "Epoch 37/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8442 - loss: 0.3286 - val_accuracy: 0.8467 - val_loss: 0.3730 - learning_rate: 0.0010\n",
      "Epoch 38/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.8627 - loss: 0.3149 - val_accuracy: 0.8400 - val_loss: 0.3659 - learning_rate: 0.0010\n",
      "Epoch 39/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.8697 - loss: 0.2983 - val_accuracy: 0.8533 - val_loss: 0.3584 - learning_rate: 0.0010\n",
      "Epoch 40/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.8671 - loss: 0.2988 - val_accuracy: 0.8533 - val_loss: 0.3580 - learning_rate: 0.0010\n",
      "Epoch 41/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.8646 - loss: 0.3004 - val_accuracy: 0.8600 - val_loss: 0.3570 - learning_rate: 0.0010\n",
      "Epoch 42/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.8756 - loss: 0.2886 - val_accuracy: 0.8533 - val_loss: 0.3480 - learning_rate: 0.0010\n",
      "Epoch 43/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.8762 - loss: 0.2888 - val_accuracy: 0.8600 - val_loss: 0.3447 - learning_rate: 0.0010\n",
      "Epoch 44/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.8722 - loss: 0.2850 - val_accuracy: 0.8667 - val_loss: 0.3426 - learning_rate: 0.0010\n",
      "Epoch 45/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.8867 - loss: 0.2748 - val_accuracy: 0.8667 - val_loss: 0.3339 - learning_rate: 0.0010\n",
      "Epoch 46/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.8862 - loss: 0.2621 - val_accuracy: 0.8733 - val_loss: 0.3335 - learning_rate: 0.0010\n",
      "Epoch 47/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.8913 - loss: 0.2504 - val_accuracy: 0.8733 - val_loss: 0.3333 - learning_rate: 0.0010\n",
      "Epoch 48/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.8992 - loss: 0.2448 - val_accuracy: 0.8667 - val_loss: 0.3304 - learning_rate: 0.0010\n",
      "Epoch 49/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8856 - loss: 0.2422 - val_accuracy: 0.8667 - val_loss: 0.3336 - learning_rate: 0.0010\n",
      "Epoch 50/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8999 - loss: 0.2298 - val_accuracy: 0.8667 - val_loss: 0.3330 - learning_rate: 0.0010\n",
      "Epoch 51/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.8948 - loss: 0.2191 - val_accuracy: 0.8667 - val_loss: 0.3246 - learning_rate: 0.0010\n",
      "Epoch 52/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.9011 - loss: 0.2240 - val_accuracy: 0.8667 - val_loss: 0.3218 - learning_rate: 0.0010\n",
      "Epoch 53/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9193 - loss: 0.2109 - val_accuracy: 0.8667 - val_loss: 0.3304 - learning_rate: 0.0010\n",
      "Epoch 54/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9053 - loss: 0.2258 - val_accuracy: 0.8733 - val_loss: 0.3302 - learning_rate: 0.0010\n",
      "Epoch 55/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9127 - loss: 0.2001 - val_accuracy: 0.8733 - val_loss: 0.3184 - learning_rate: 0.0010\n",
      "Epoch 56/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9244 - loss: 0.2013 - val_accuracy: 0.8800 - val_loss: 0.3231 - learning_rate: 0.0010\n",
      "Epoch 57/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9194 - loss: 0.1929 - val_accuracy: 0.8733 - val_loss: 0.3253 - learning_rate: 0.0010\n",
      "Epoch 58/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9191 - loss: 0.1894 - val_accuracy: 0.8733 - val_loss: 0.3245 - learning_rate: 0.0010\n",
      "Epoch 59/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9394 - loss: 0.1857 - val_accuracy: 0.8867 - val_loss: 0.3149 - learning_rate: 0.0010\n",
      "Epoch 60/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9098 - loss: 0.1861 - val_accuracy: 0.8867 - val_loss: 0.3210 - learning_rate: 0.0010\n",
      "Epoch 61/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9232 - loss: 0.1731 - val_accuracy: 0.8733 - val_loss: 0.3239 - learning_rate: 0.0010\n",
      "Epoch 62/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9444 - loss: 0.1641 - val_accuracy: 0.8867 - val_loss: 0.3163 - learning_rate: 0.0010\n",
      "Epoch 63/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9363 - loss: 0.1693 - val_accuracy: 0.8867 - val_loss: 0.3168 - learning_rate: 0.0010\n",
      "Epoch 64/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9275 - loss: 0.1685 - val_accuracy: 0.8800 - val_loss: 0.3191 - learning_rate: 0.0010\n",
      "Epoch 65/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9393 - loss: 0.1564 - val_accuracy: 0.8933 - val_loss: 0.3130 - learning_rate: 0.0010\n",
      "Epoch 66/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.9421 - loss: 0.1527 - val_accuracy: 0.9000 - val_loss: 0.3112 - learning_rate: 0.0010\n",
      "Epoch 67/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9271 - loss: 0.1498 - val_accuracy: 0.9000 - val_loss: 0.3153 - learning_rate: 0.0010\n",
      "Epoch 68/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9566 - loss: 0.1299 - val_accuracy: 0.9067 - val_loss: 0.3132 - learning_rate: 0.0010\n",
      "Epoch 69/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9469 - loss: 0.1404 - val_accuracy: 0.9000 - val_loss: 0.3208 - learning_rate: 0.0010\n",
      "Epoch 70/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9454 - loss: 0.1443 - val_accuracy: 0.9000 - val_loss: 0.3157 - learning_rate: 0.0010\n",
      "Epoch 71/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9355 - loss: 0.1435 - val_accuracy: 0.9000 - val_loss: 0.3126 - learning_rate: 0.0010\n",
      "Epoch 72/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9507 - loss: 0.1315 - val_accuracy: 0.9000 - val_loss: 0.3208 - learning_rate: 0.0010\n",
      "Epoch 73/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9459 - loss: 0.1306 - val_accuracy: 0.9000 - val_loss: 0.3274 - learning_rate: 0.0010\n",
      "Epoch 74/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9628 - loss: 0.1175 - val_accuracy: 0.9000 - val_loss: 0.3230 - learning_rate: 0.0010\n",
      "Epoch 75/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9688 - loss: 0.1129 - val_accuracy: 0.9000 - val_loss: 0.3308 - learning_rate: 0.0010\n",
      "Epoch 76/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9589 - loss: 0.1231 - val_accuracy: 0.9000 - val_loss: 0.3335 - learning_rate: 0.0010\n",
      "Epoch 77/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9614 - loss: 0.1175 - val_accuracy: 0.9000 - val_loss: 0.3293 - learning_rate: 0.0010\n",
      "Epoch 78/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9597 - loss: 0.1057 - val_accuracy: 0.9067 - val_loss: 0.3294 - learning_rate: 0.0010\n",
      "Epoch 79/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9605 - loss: 0.1072 - val_accuracy: 0.9000 - val_loss: 0.3419 - learning_rate: 0.0010\n",
      "Epoch 80/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9704 - loss: 0.1071 - val_accuracy: 0.9000 - val_loss: 0.3497 - learning_rate: 0.0010\n",
      "Epoch 81/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9738 - loss: 0.0969 - val_accuracy: 0.9000 - val_loss: 0.3536 - learning_rate: 0.0010\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8950 - loss: 0.2392\n",
      "Test loss: 0.24669751524925232 - Test accuracy: 0.8933333158493042\n"
     ]
    }
   ],
   "source": [
    "### Augment\n",
    "import tensorflow as tf\n",
    "from silence_tensorflow import silence_tensorflow\n",
    "silence_tensorflow()\n",
    "from sklearn.datasets import fetch_openml\n",
    "from tensorflow.keras.layers import Dense, Input, Embedding, Flatten, Concatenate, IntegerLookup, StringLookup\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable(package=\"Custom\", name=\"LogTransform\")\n",
    "class LogTransform(tf.keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        return tf.math.log1p(inputs)\n",
    "\n",
    "    def get_config(self):  # Required for serialization\n",
    "        return super().get_config()\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable(package=\"Custom\", name=\"LogTransform\")\n",
    "class Standardize(tf.keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        return (inputs - tf.reduce_mean(inputs)) / tf.math.reduce_std(inputs)\n",
    "\n",
    "    def get_config(self):  # Required for serialization\n",
    "        return super().get_config()\n",
    "\n",
    "\n",
    "class CreditDataPreprocessor:\n",
    "    def __init__(self, discrete_features, categorical_features, continuous_features):\n",
    "        self.discrete_features = discrete_features\n",
    "        self.categorical_features = categorical_features\n",
    "        self.continuous_features = continuous_features\n",
    "        \n",
    "        # Initialize encoders\n",
    "        self.ordinal_encoders = {\n",
    "            col: IntegerLookup(output_mode='int', num_oov_indices=1) \n",
    "            for col in discrete_features\n",
    "        }\n",
    "        self.categorical_encoders = {\n",
    "            col: StringLookup(output_mode='int', num_oov_indices=1) \n",
    "            for col in categorical_features\n",
    "        }\n",
    "        \n",
    "    def adapt(self, dataset):\n",
    "        \"\"\"Adapt all encoders to the data\"\"\"\n",
    "        for batch in dataset:\n",
    "            features, _ = batch\n",
    "            for col in self.discrete_features:\n",
    "                self.ordinal_encoders[col].adapt(features[col])\n",
    "            for col in self.categorical_features:\n",
    "                self.categorical_encoders[col].adapt(features[col])\n",
    "    \n",
    "    def preprocess_batch(self, features, labels):\n",
    "        \"\"\"Transform a batch of data\"\"\"\n",
    "        inputs = {\n",
    "            **{f\"{col}_input\": features[col] for col in self.continuous_features},\n",
    "            **{f\"{col}_input\": tf.cast(features[col], tf.int32) for col in self.discrete_features},\n",
    "            **{f\"{col}_input\": features[col] for col in self.categorical_features}\n",
    "        }\n",
    "        return inputs, labels\n",
    "    \n",
    "    def prepare_dataset(self, dataset):\n",
    "        \"\"\"Prepare a dataset for training\"\"\"\n",
    "        return dataset.map(self.preprocess_batch).cache()\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable(package=\"Custom\", name=\"DiscreteFeatureEncoder\")\n",
    "class DiscreteFeatureEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, encoder, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Implement build method to avoid warnings\n",
    "        # No weights to initialize, but we can call this to mark the layer as built\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.cast(self.encoder(inputs), tf.float32)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        # Save the encoder configuration\n",
    "        config.update({\n",
    "            'encoder_config': tf.keras.layers.serialize(self.encoder),\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        # Deserialize the encoder\n",
    "        encoder_config = config.pop('encoder_config')\n",
    "        encoder = tf.keras.layers.deserialize(encoder_config)\n",
    "        return cls(encoder, **config)\n",
    "\n",
    "\n",
    "\n",
    "class CreditRiskModel(tf.keras.Model):  # Inherit from tf.keras.Model\n",
    "    def __init__(self, preprocessor, embedding_size=8, **kwargs):\n",
    "        super().__init__(**kwargs)  # Ensure proper initialization\n",
    "        self.preprocessor = preprocessor\n",
    "        self.embedding_size = embedding_size\n",
    "        self.model = self.build_model()  # Store Keras model\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"Builds and returns a Keras model\"\"\"\n",
    "        continuous_inputs = {\n",
    "            col: Input(shape=(1,), dtype=tf.float32, name=f\"{col}_input\") \n",
    "            for col in self.preprocessor.continuous_features\n",
    "        }\n",
    "        discrete_inputs = {\n",
    "            col: Input(shape=(1,), dtype=tf.int32, name=f\"{col}_input\") \n",
    "            for col in self.preprocessor.discrete_features\n",
    "        }\n",
    "        categorical_inputs = {\n",
    "            col: Input(shape=(1,), dtype=tf.string, name=f\"{col}_input\") \n",
    "            for col in self.preprocessor.categorical_features\n",
    "        }\n",
    "        \n",
    "        processed_features = self._process_features(\n",
    "            continuous_inputs, discrete_inputs, categorical_inputs)\n",
    "        \n",
    "        x = Dense(128, activation='relu', kernel_initializer='he_normal')(processed_features)\n",
    "        x = tf.keras.layers.Dropout(0.1)(x)\n",
    "        x = Dense(64, activation='relu', kernel_initializer='he_normal')(x)\n",
    "        x = tf.keras.layers.Dropout(0.1)(x)\n",
    "        output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "        model_inputs = list(continuous_inputs.values()) + list(discrete_inputs.values()) + list(categorical_inputs.values())\n",
    "        return Model(inputs=model_inputs, outputs=output)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Forward pass for Keras\"\"\"\n",
    "        return self.model(inputs)\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"Required for serialization\"\"\"\n",
    "        return {\n",
    "            \"embedding_size\": self.embedding_size,\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        \"\"\"Load model from config\"\"\"\n",
    "        return cls(**config)\n",
    "\n",
    "    def save_model(self, path=\"logistic_credit_model_tf.keras\"):\n",
    "        \"\"\"Save the model properly\"\"\"\n",
    "        self.model.save(path)  # Save only the inner Keras model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def _process_features(self, continuous_inputs, discrete_inputs, categorical_inputs):\n",
    "        log_transform = LogTransform()\n",
    "        standardize = Standardize()\n",
    "    \n",
    "        # Process continuous features\n",
    "        processed_continuous = [\n",
    "            standardize(log_transform(continuous_inputs[col]))\n",
    "            for col in self.preprocessor.continuous_features\n",
    "        ]\n",
    "        \n",
    "        processed_discrete = [\n",
    "            DiscreteFeatureEncoder(self.preprocessor.ordinal_encoders[col])(discrete_inputs[col])\n",
    "            for col in self.preprocessor.discrete_features\n",
    "        ]\n",
    "\n",
    "    \n",
    "        # Process categorical features\n",
    "        embedded_features = [\n",
    "            Flatten()(Embedding(\n",
    "                input_dim=self.preprocessor.categorical_encoders[col].vocabulary_size(),\n",
    "                output_dim=self.embedding_size\n",
    "            )(self.preprocessor.categorical_encoders[col](categorical_inputs[col])))\n",
    "            for col in self.preprocessor.categorical_features\n",
    "        ]\n",
    "    \n",
    "        return Concatenate()(processed_continuous + processed_discrete + embedded_features)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def create_tf_datasets(X, y, train_size=0.7, val_size=0.15, batch_size=128, seed=None):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(X), y))\n",
    "    dataset = dataset.shuffle(buffer_size=len(X), seed=seed)\n",
    "    n = len(X)\n",
    "    train_size_n = int(n * train_size)\n",
    "    val_size_n = int(n * val_size)\n",
    "    \n",
    "    train_dataset = dataset.take(train_size_n)\n",
    "    val_dataset = dataset.skip(train_size_n).take(val_size_n)\n",
    "    test_dataset = dataset.skip(train_size_n + val_size_n)\n",
    "    \n",
    "    return (\n",
    "        train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE),\n",
    "        val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE),\n",
    "        test_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "# Load and prepare data\n",
    "credit_data = fetch_openml(name='credit-g', version=1, as_frame=True)\n",
    "X = credit_data.data\n",
    "y = credit_data.target.map({'good': 1, 'bad': 0}).values\n",
    "\n",
    "# Define feature columns\n",
    "discrete_features = ['installment_commitment', 'residence_since', 'num_dependents', 'existing_credits']\n",
    "categorical_features = X.select_dtypes(exclude='number').columns.tolist()\n",
    "continuous_features = ['duration', 'credit_amount']\n",
    "\n",
    "# Create datasets\n",
    "seed = 2025\n",
    "train_dataset_raw, val_dataset_raw, test_dataset_raw = create_tf_datasets(X, y, seed=seed)\n",
    "\n",
    "# Initialize and adapt preprocessor\n",
    "preprocessor = CreditDataPreprocessor(\n",
    "    discrete_features=discrete_features,\n",
    "    categorical_features=categorical_features,\n",
    "    continuous_features=continuous_features\n",
    ")\n",
    "preprocessor.adapt(train_dataset_raw)\n",
    "\n",
    "# Prepare datasets\n",
    "train_dataset = preprocessor.prepare_dataset(train_dataset_raw)\n",
    "val_dataset = preprocessor.prepare_dataset(val_dataset_raw)\n",
    "test_dataset = preprocessor.prepare_dataset(test_dataset_raw)\n",
    "\n",
    "# Create and compile model\n",
    "credit_model = CreditRiskModel(preprocessor)  \n",
    "credit_model.model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                           loss='binary_crossentropy',\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=15, restore_best_weights=True, monitor='val_loss'),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=15, min_lr=1e-6),\n",
    "    ModelCheckpoint('best_logistic_credit_model_tf.keras', monitor='val_loss', save_best_only=True)\n",
    "]\n",
    "\n",
    "credit_model.model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=200,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "test_loss, test_acc = credit_model.model.evaluate(test_dataset)\n",
    "print(f\"Test loss: {test_loss} - Test accuracy: {test_acc}\")\n",
    "credit_model.save_model()  # Now works correctly\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_model.save_model()  # Now works correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "custom_objects = {\n",
    "    \"LogTransform\": LogTransform,\n",
    "    \"Standardize\": Standardize,\n",
    "    \"DiscreteFeatureEncoder\": DiscreteFeatureEncoder\n",
    "}\n",
    "\n",
    "model = load_model(\"logistic_credit_model_tf.keras\", custom_objects=custom_objects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'kerastuner'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkerastuner\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mkt\u001b[39;00m \u001b[38;5;66;03m# Import kerastuner\u001b[39;00m\n\u001b[0;32m     11\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m pip install keras-tuner\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;129m@tf\u001b[39m\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mregister_keras_serializable(package\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCustom\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLogTransform\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLogTransform\u001b[39;00m(tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mLayer):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'kerastuner'"
     ]
    }
   ],
   "source": [
    "### Augment\n",
    "import tensorflow as tf\n",
    "!pip install silence_tensorflow\n",
    "from silence_tensorflow import silence_tensorflow\n",
    "silence_tensorflow()\n",
    "from sklearn.datasets import fetch_openml\n",
    "from tensorflow.keras.layers import Dense, Input, Embedding, Flatten, Concatenate, IntegerLookup, StringLookup\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "! pip install keras-tuner\n",
    "import keras_tuner as kt # Import kerastuner\n",
    "\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable(package=\"Custom\", name=\"LogTransform\")\n",
    "class LogTransform(tf.keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        return tf.math.log1p(inputs)\n",
    "\n",
    "    def get_config(self):  # Required for serialization\n",
    "        return super().get_config()\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable(package=\"Custom\", name=\"LogTransform\")\n",
    "class Standardize(tf.keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        return (inputs - tf.reduce_mean(inputs)) / tf.math.reduce_std(inputs)\n",
    "\n",
    "    def get_config(self):  # Required for serialization\n",
    "        return super().get_config()\n",
    "\n",
    "\n",
    "class CreditDataPreprocessor:\n",
    "    def __init__(self, discrete_features, categorical_features, continuous_features):\n",
    "        self.discrete_features = discrete_features\n",
    "        self.categorical_features = categorical_features\n",
    "        self.continuous_features = continuous_features\n",
    "        \n",
    "        # Initialize encoders\n",
    "        self.ordinal_encoders = {\n",
    "            col: IntegerLookup(output_mode='int', num_oov_indices=1) \n",
    "            for col in discrete_features\n",
    "        }\n",
    "        self.categorical_encoders = {\n",
    "            col: StringLookup(output_mode='int', num_oov_indices=1) \n",
    "            for col in categorical_features\n",
    "        }\n",
    "        \n",
    "    def adapt(self, dataset):\n",
    "        \"\"\"Adapt all encoders to the data\"\"\"\n",
    "        for batch in dataset:\n",
    "            features, _ = batch\n",
    "            for col in self.discrete_features:\n",
    "                self.ordinal_encoders[col].adapt(features[col])\n",
    "            for col in self.categorical_features:\n",
    "                self.categorical_encoders[col].adapt(features[col])\n",
    "    \n",
    "    def preprocess_batch(self, features, labels):\n",
    "        \"\"\"Transform a batch of data\"\"\"\n",
    "        inputs = {\n",
    "            **{f\"{col}_input\": features[col] for col in self.continuous_features},\n",
    "            **{f\"{col}_input\": tf.cast(features[col], tf.int32) for col in self.discrete_features},\n",
    "            **{f\"{col}_input\": features[col] for col in self.categorical_features}\n",
    "        }\n",
    "        return inputs, labels\n",
    "    \n",
    "    def prepare_dataset(self, dataset):\n",
    "        \"\"\"Prepare a dataset for training\"\"\"\n",
    "        return dataset.map(self.preprocess_batch).cache()\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable(package=\"Custom\", name=\"DiscreteFeatureEncoder\")\n",
    "class DiscreteFeatureEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, encoder, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Implement build method to avoid warnings\n",
    "        # No weights to initialize, but we can call this to mark the layer as built\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.cast(self.encoder(inputs), tf.float32)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        # Save the encoder configuration\n",
    "        config.update({\n",
    "            'encoder_config': tf.keras.layers.serialize(self.encoder),\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        # Deserialize the encoder\n",
    "        encoder_config = config.pop('encoder_config')\n",
    "        encoder = tf.keras.layers.deserialize(encoder_config)\n",
    "        return cls(encoder, **config)\n",
    "\n",
    "\n",
    "\n",
    "class CreditRiskModel(tf.keras.Model):  # Inherit from tf.keras.Model\n",
    "    def __init__(self, preprocessor, embedding_size=8, **kwargs):\n",
    "        super().__init__(**kwargs)  # Ensure proper initialization\n",
    "        self.preprocessor = preprocessor\n",
    "        self.embedding_size = embedding_size\n",
    "        self.model = self.build_model()  # Store Keras model\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"Builds and returns a Keras model\"\"\"\n",
    "        continuous_inputs = {\n",
    "            col: Input(shape=(1,), dtype=tf.float32, name=f\"{col}_input\") \n",
    "            for col in self.preprocessor.continuous_features\n",
    "        }\n",
    "        discrete_inputs = {\n",
    "            col: Input(shape=(1,), dtype=tf.int32, name=f\"{col}_input\") \n",
    "            for col in self.preprocessor.discrete_features\n",
    "        }\n",
    "        categorical_inputs = {\n",
    "            col: Input(shape=(1,), dtype=tf.string, name=f\"{col}_input\") \n",
    "            for col in self.preprocessor.categorical_features\n",
    "        }\n",
    "        \n",
    "        processed_features = self._process_features(\n",
    "            continuous_inputs, discrete_inputs, categorical_inputs)\n",
    "        \n",
    "        x = Dense(128, activation='relu', kernel_initializer='he_normal')(processed_features)\n",
    "        x = tf.keras.layers.Dropout(0.1)(x)\n",
    "        x = Dense(64, activation='relu', kernel_initializer='he_normal')(x)\n",
    "        x = tf.keras.layers.Dropout(0.1)(x)\n",
    "        output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "        model_inputs = list(continuous_inputs.values()) + list(discrete_inputs.values()) + list(categorical_inputs.values())\n",
    "        return Model(inputs=model_inputs, outputs=output)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Forward pass for Keras\"\"\"\n",
    "        return self.model(inputs)\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"Required for serialization\"\"\"\n",
    "        return {\n",
    "            \"embedding_size\": self.embedding_size,\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        \"\"\"Load model from config\"\"\"\n",
    "        return cls(**config)\n",
    "\n",
    "    def save_model(self, path=\"logistic_credit_model_tf.keras\"):\n",
    "        \"\"\"Save the model properly\"\"\"\n",
    "        self.model.save(path)  # Save only the inner Keras model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # Modify _process_features to accept preprocessor as an argument\n",
    "    def _process_features(self, continuous_inputs, discrete_inputs, categorical_inputs):  # Note the 'self'\n",
    "        log_transform = LogTransform()\n",
    "        standardize = Standardize()\n",
    "\n",
    "        processed_continuous = [\n",
    "            standardize(log_transform(continuous_inputs[col]))\n",
    "            for col in self.preprocessor.continuous_features\n",
    "        ]\n",
    "\n",
    "        processed_discrete = [\n",
    "            DiscreteFeatureEncoder(self.preprocessor.ordinal_encoders[col])(discrete_inputs[col])\n",
    "            for col in self.preprocessor.discrete_features\n",
    "        ]\n",
    "\n",
    "        embedded_features = [\n",
    "            Flatten()(Embedding(\n",
    "                input_dim=self.preprocessor.categorical_encoders[col].vocabulary_size(),\n",
    "                output_dim=self.embedding_size\n",
    "            )(self.preprocessor.categorical_encoders[col](categorical_inputs[col])))\n",
    "            for col in self.preprocessor.categorical_features\n",
    "        ]\n",
    "\n",
    "        return Concatenate()(processed_continuous + processed_discrete + embedded_features)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def create_tf_datasets(X, y, train_size=0.7, val_size=0.15, batch_size=128, seed=None):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(X), y))\n",
    "    dataset = dataset.shuffle(buffer_size=len(X), seed=seed)\n",
    "    n = len(X)\n",
    "    train_size_n = int(n * train_size)\n",
    "    val_size_n = int(n * val_size)\n",
    "    \n",
    "    train_dataset = dataset.take(train_size_n)\n",
    "    val_dataset = dataset.skip(train_size_n).take(val_size_n)\n",
    "    test_dataset = dataset.skip(train_size_n + val_size_n)\n",
    "    \n",
    "    return (\n",
    "        train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE),\n",
    "        val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE),\n",
    "        test_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "\n",
    "# Hyperparameter search space\n",
    "def build_model(hp):\n",
    "    # Instantiate the model\n",
    "    model_instance = CreditRiskModel(preprocessor)\n",
    "\n",
    "    continuous_inputs = {\n",
    "        col: Input(shape=(1,), dtype=tf.float32, name=f\"{col}_input\")\n",
    "        for col in preprocessor.continuous_features\n",
    "    }\n",
    "    discrete_inputs = {\n",
    "        col: Input(shape=(1,), dtype=tf.int32, name=f\"{col}_input\")\n",
    "        for col in preprocessor.discrete_features\n",
    "    }\n",
    "    categorical_inputs = {\n",
    "        col: Input(shape=(1,), dtype=tf.string, name=f\"{col}_input\")\n",
    "        for col in preprocessor.categorical_features\n",
    "    }\n",
    "\n",
    "    # Call the instance method\n",
    "    processed_features = model_instance._process_features(\n",
    "        continuous_inputs, discrete_inputs, categorical_inputs\n",
    "    )\n",
    "\n",
    "    # Expanded search space\n",
    "    hp_units1 = hp.Int('units1', min_value=32, max_value=256, step=16)  # Larger range for units\n",
    "    hp_units2 = hp.Int('units2', min_value=32, max_value=256, step=16)   # Larger range for units\n",
    "    hp_dropout = hp.Float('dropout', min_value=0.0, max_value=0.5, step=0.05)  # Wider range for dropout\n",
    "    hp_learning_rate = hp.Float('learning_rate', min_value=1e-5, max_value=1e-2, sampling='log')  # Wider range for learning rate\n",
    "\n",
    "    x = Dense(hp_units1, activation='relu', kernel_initializer='he_normal')(processed_features)\n",
    "    x = tf.keras.layers.Dropout(hp_dropout)(x)\n",
    "    x = Dense(hp_units2, activation='relu', kernel_initializer='he_normal')(x)\n",
    "    x = tf.keras.layers.Dropout(hp_dropout)(x)\n",
    "    output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model_inputs = list(continuous_inputs.values()) + list(discrete_inputs.values()) + list(categorical_inputs.values())\n",
    "    model = Model(inputs=model_inputs, outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=hp_learning_rate),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Load and prepare data\n",
    "credit_data = fetch_openml(name='credit-g', version=1, as_frame=True)\n",
    "X = credit_data.data\n",
    "y = credit_data.target.map({'good': 1, 'bad': 0}).values\n",
    "\n",
    "# Define feature columns\n",
    "discrete_features = ['installment_commitment', 'residence_since', 'num_dependents', 'existing_credits']\n",
    "categorical_features = X.select_dtypes(exclude='number').columns.tolist()\n",
    "continuous_features = ['duration', 'credit_amount']\n",
    "\n",
    "# Create datasets\n",
    "seed = 2025\n",
    "train_dataset_raw, val_dataset_raw, test_dataset_raw = create_tf_datasets(X, y, seed=seed)\n",
    "\n",
    "# Initialize and adapt preprocessor\n",
    "preprocessor = CreditDataPreprocessor(\n",
    "    discrete_features=discrete_features,\n",
    "    categorical_features=categorical_features,\n",
    "    continuous_features=continuous_features\n",
    ")\n",
    "preprocessor.adapt(train_dataset_raw)\n",
    "\n",
    "# Prepare datasets\n",
    "train_dataset = preprocessor.prepare_dataset(train_dataset_raw)\n",
    "val_dataset = preprocessor.prepare_dataset(val_dataset_raw)\n",
    "test_dataset = preprocessor.prepare_dataset(test_dataset_raw)\n",
    "\n",
    "# Create and compile model\n",
    "credit_model = CreditRiskModel(preprocessor)  \n",
    "credit_model.model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                           loss='binary_crossentropy',\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "\n",
    "tuner = kt.RandomSearch(\n",
    "    hypermodel=build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=200,  # Adjust the number of trials as needed\n",
    "    executions_per_trial=1,\n",
    "    directory='my_hyperparameter_tuning',\n",
    "    project_name='credit_risk'\n",
    ")\n",
    "\n",
    "\n",
    "# Early stopping and learning rate reduction (remains unchanged, but monitoring 'val_accuracy' might be better for the objective)\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=15, restore_best_weights=True, monitor='val_accuracy'), # Changed to monitor val_accuracy\n",
    "    ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=15, min_lr=1e-6), # Changed to monitor val_accuracy\n",
    "    ModelCheckpoint('best_logistic_credit_model_tf.keras', monitor='val_accuracy', save_best_only=True) # Changed to monitor val_accuracy\n",
    "]\n",
    "\n",
    "credit_model.model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=200,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "tuner.search_space_summary()\n",
    "tuner.search(train_dataset, epochs=200, validation_data=val_dataset, callbacks=callbacks)\n",
    "\n",
    "best_hp = tuner.get_best_hyperparameters(1)[0]\n",
    "print(f\"Best hyperparameters: {best_hp.values}\")\n",
    "\n",
    "best_model = tuner.hypermodel.build(best_hp)\n",
    "best_model.fit(train_dataset, epochs=200, validation_data=val_dataset, callbacks=callbacks)\n",
    "test_loss, test_acc = best_model.evaluate(test_dataset)\n",
    "print(f\"Test loss: {test_loss} - Test accuracy: {test_acc}\")\n",
    "\n",
    "best_model.save('best_model_with_tuner.keras')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
