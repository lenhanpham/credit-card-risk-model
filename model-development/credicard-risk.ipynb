{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: silence_tensorflow in d:\\softwares\\linux\\anaconda3\\envs\\ml\\lib\\site-packages (1.2.3)\n",
      "Epoch 1/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 428ms/step - accuracy: 0.4428 - loss: 0.7335 - val_accuracy: 0.6600 - val_loss: 0.6626 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7008 - loss: 0.6151 - val_accuracy: 0.6600 - val_loss: 0.6729 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.7021 - loss: 0.6007 - val_accuracy: 0.6600 - val_loss: 0.6517 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 80ms/step - accuracy: 0.7093 - loss: 0.5816 - val_accuracy: 0.6700 - val_loss: 0.6377 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.7152 - loss: 0.5720 - val_accuracy: 0.6500 - val_loss: 0.6274 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 88ms/step - accuracy: 0.7202 - loss: 0.5609 - val_accuracy: 0.6800 - val_loss: 0.6145 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.7347 - loss: 0.5451 - val_accuracy: 0.6800 - val_loss: 0.6014 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.7343 - loss: 0.5290 - val_accuracy: 0.6900 - val_loss: 0.5852 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.7404 - loss: 0.5093 - val_accuracy: 0.7100 - val_loss: 0.5743 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.7462 - loss: 0.4979 - val_accuracy: 0.7300 - val_loss: 0.5694 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.7499 - loss: 0.4907 - val_accuracy: 0.7200 - val_loss: 0.5605 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.7440 - loss: 0.4851 - val_accuracy: 0.7300 - val_loss: 0.5475 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.7549 - loss: 0.4719 - val_accuracy: 0.7200 - val_loss: 0.5349 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.7624 - loss: 0.4726 - val_accuracy: 0.7300 - val_loss: 0.5201 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.7663 - loss: 0.4683 - val_accuracy: 0.7200 - val_loss: 0.5028 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.7844 - loss: 0.4544 - val_accuracy: 0.7200 - val_loss: 0.4911 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.7712 - loss: 0.4552 - val_accuracy: 0.7300 - val_loss: 0.4818 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 85ms/step - accuracy: 0.7707 - loss: 0.4483 - val_accuracy: 0.7300 - val_loss: 0.4736 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.7832 - loss: 0.4403 - val_accuracy: 0.7600 - val_loss: 0.4666 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.7811 - loss: 0.4386 - val_accuracy: 0.7500 - val_loss: 0.4587 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.7937 - loss: 0.4337 - val_accuracy: 0.7500 - val_loss: 0.4508 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.7957 - loss: 0.4228 - val_accuracy: 0.7600 - val_loss: 0.4443 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.7838 - loss: 0.4352 - val_accuracy: 0.7800 - val_loss: 0.4386 - learning_rate: 0.0010\n",
      "Epoch 24/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 85ms/step - accuracy: 0.8011 - loss: 0.4211 - val_accuracy: 0.7800 - val_loss: 0.4336 - learning_rate: 0.0010\n",
      "Epoch 25/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.8125 - loss: 0.4156 - val_accuracy: 0.7900 - val_loss: 0.4283 - learning_rate: 0.0010\n",
      "Epoch 26/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.8146 - loss: 0.4118 - val_accuracy: 0.7800 - val_loss: 0.4251 - learning_rate: 0.0010\n",
      "Epoch 27/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.8140 - loss: 0.3969 - val_accuracy: 0.7800 - val_loss: 0.4221 - learning_rate: 0.0010\n",
      "Epoch 28/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 84ms/step - accuracy: 0.8089 - loss: 0.4002 - val_accuracy: 0.7800 - val_loss: 0.4190 - learning_rate: 0.0010\n",
      "Epoch 29/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.8176 - loss: 0.3882 - val_accuracy: 0.7700 - val_loss: 0.4189 - learning_rate: 0.0010\n",
      "Epoch 30/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.8161 - loss: 0.3971 - val_accuracy: 0.7800 - val_loss: 0.4162 - learning_rate: 0.0010\n",
      "Epoch 31/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.8225 - loss: 0.3897 - val_accuracy: 0.7900 - val_loss: 0.4138 - learning_rate: 0.0010\n",
      "Epoch 32/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.8356 - loss: 0.3845 - val_accuracy: 0.7900 - val_loss: 0.4118 - learning_rate: 0.0010\n",
      "Epoch 33/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8252 - loss: 0.3652 - val_accuracy: 0.7800 - val_loss: 0.4140 - learning_rate: 0.0010\n",
      "Epoch 34/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8273 - loss: 0.3746 - val_accuracy: 0.7900 - val_loss: 0.4144 - learning_rate: 0.0010\n",
      "Epoch 35/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.8399 - loss: 0.3627 - val_accuracy: 0.7800 - val_loss: 0.4096 - learning_rate: 0.0010\n",
      "Epoch 36/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.8480 - loss: 0.3596 - val_accuracy: 0.7900 - val_loss: 0.4093 - learning_rate: 0.0010\n",
      "Epoch 37/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8406 - loss: 0.3559 - val_accuracy: 0.8000 - val_loss: 0.4106 - learning_rate: 0.0010\n",
      "Epoch 38/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.8584 - loss: 0.3410 - val_accuracy: 0.8000 - val_loss: 0.4078 - learning_rate: 0.0010\n",
      "Epoch 39/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.8384 - loss: 0.3522 - val_accuracy: 0.8200 - val_loss: 0.4071 - learning_rate: 0.0010\n",
      "Epoch 40/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.8595 - loss: 0.3352 - val_accuracy: 0.8200 - val_loss: 0.4005 - learning_rate: 0.0010\n",
      "Epoch 41/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.8404 - loss: 0.3342 - val_accuracy: 0.8400 - val_loss: 0.3992 - learning_rate: 0.0010\n",
      "Epoch 42/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.8709 - loss: 0.3150 - val_accuracy: 0.8100 - val_loss: 0.3921 - learning_rate: 0.0010\n",
      "Epoch 43/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8682 - loss: 0.3063 - val_accuracy: 0.8400 - val_loss: 0.3978 - learning_rate: 0.0010\n",
      "Epoch 44/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8714 - loss: 0.3161 - val_accuracy: 0.8400 - val_loss: 0.3924 - learning_rate: 0.0010\n",
      "Epoch 45/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8696 - loss: 0.3034 - val_accuracy: 0.8400 - val_loss: 0.3939 - learning_rate: 0.0010\n",
      "Epoch 46/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.8761 - loss: 0.3001 - val_accuracy: 0.8300 - val_loss: 0.3862 - learning_rate: 0.0010\n",
      "Epoch 47/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8707 - loss: 0.3038 - val_accuracy: 0.8500 - val_loss: 0.3882 - learning_rate: 0.0010\n",
      "Epoch 48/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.8736 - loss: 0.2960 - val_accuracy: 0.8400 - val_loss: 0.3805 - learning_rate: 0.0010\n",
      "Epoch 49/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.8845 - loss: 0.2682 - val_accuracy: 0.8400 - val_loss: 0.3802 - learning_rate: 0.0010\n",
      "Epoch 50/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8951 - loss: 0.2652 - val_accuracy: 0.8500 - val_loss: 0.3848 - learning_rate: 0.0010\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.9000 - loss: 0.2862\n",
      "Test loss: 0.28622475266456604 - Test accuracy: 0.8999999761581421\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "!pip install silence_tensorflow\n",
    "from silence_tensorflow import silence_tensorflow\n",
    "silence_tensorflow()\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from tensorflow.keras.layers import Dense, Input, Embedding, Flatten, Concatenate, BatchNormalization, IntegerLookup, StringLookup\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "# Load dataset\n",
    "credit_data = fetch_openml(name='credit-g', version=1, as_frame=True)\n",
    "X = credit_data.data\n",
    "y = credit_data.target.map({'good': 1, 'bad': 0}).values\n",
    "\n",
    "# Define feature columns\n",
    "discrete_features = ['installment_commitment', 'residence_since', 'num_dependents', 'existing_credits']\n",
    "categorical_features = X.select_dtypes(exclude='number').columns.tolist()\n",
    "continuous_features = ['duration', 'credit_amount']\n",
    "\n",
    "# Create TensorFlow datasets with 80/10/10 split\n",
    "def create_tf_datasets(X, y, train_size=0.8, val_size=0.1, batch_size=128, seed=None):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(X), y))\n",
    "    dataset = dataset.shuffle(buffer_size=len(X), seed=seed)  # Configurable seed\n",
    "    n = len(X)\n",
    "    train_size = int(n * train_size)\n",
    "    val_size = int(n * val_size)\n",
    "    train_dataset = dataset.take(train_size)\n",
    "    val_dataset = dataset.skip(train_size).take(val_size)\n",
    "    test_dataset = dataset.skip(train_size + val_size)\n",
    "    return (\n",
    "        train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE),\n",
    "        val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE),\n",
    "        test_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "# Try different seeds (None for unseeded, or specific values)\n",
    "seed = 2025  # Change to None for randomness, or test 0, 1, 42, etc.\n",
    "train_dataset_raw, val_dataset_raw, test_dataset_raw = create_tf_datasets(X, y, seed=seed)\n",
    "\n",
    "# Adapt lookup layers\n",
    "def adapt_preprocessing_layers(dataset):\n",
    "    ordinal_encoders = {col: IntegerLookup(output_mode='int', num_oov_indices=1) for col in discrete_features}\n",
    "    categorical_encoders = {col: StringLookup(output_mode='int', num_oov_indices=1) for col in categorical_features}\n",
    "\n",
    "    for batch in dataset:\n",
    "        features, _ = batch\n",
    "        for col in discrete_features:\n",
    "            ordinal_encoders[col].adapt(features[col])\n",
    "        for col in categorical_features:\n",
    "            categorical_encoders[col].adapt(features[col])\n",
    "\n",
    "    return ordinal_encoders, categorical_encoders\n",
    "\n",
    "ordinal_encoders, categorical_encoders = adapt_preprocessing_layers(train_dataset_raw)\n",
    "\n",
    "def log1p_with_shape(x):\n",
    "    return tf.math.log1p(x)\n",
    "\n",
    "def cast_to_float_with_shape(x):\n",
    "    return tf.cast(x, tf.float32)\n",
    "\n",
    "\n",
    "# Build model with preprocessing\n",
    "def build_preprocessing_model():\n",
    "    continuous_inputs = {col: Input(shape=(1,), dtype=tf.float32, name=f\"{col}_input\") for col in continuous_features}\n",
    "    discrete_inputs = {col: Input(shape=(1,), dtype=tf.int32, name=f\"{col}_input\") for col in discrete_features}\n",
    "    categorical_inputs = {col: Input(shape=(1,), dtype=tf.string, name=f\"{col}_input\") for col in categorical_features}\n",
    "\n",
    "    processed_continuous = [\n",
    "        BatchNormalization(momentum=0.1, epsilon=1e-5)(\n",
    "            tf.keras.layers.Lambda(\n",
    "                log1p_with_shape,\n",
    "                output_shape=(1,),\n",
    "                name=f'log1p_lambda_{col}'\n",
    "            )(continuous_inputs[col])\n",
    "        ) for col in continuous_features\n",
    "    ]\n",
    "\n",
    "\n",
    "    processed_discrete = [\n",
    "        tf.keras.layers.Lambda(\n",
    "            lambda x: cast_to_float_with_shape(ordinal_encoders[col](x)),\n",
    "            output_shape=(1,),\n",
    "            name=f'cast_lambda_{col}'\n",
    "        )(discrete_inputs[col])\n",
    "        for col in discrete_features\n",
    "    ]\n",
    "\n",
    "    embedding_size = 8\n",
    "    embedded_features = [\n",
    "        Flatten()(Embedding(input_dim=categorical_encoders[col].vocabulary_size(), output_dim=embedding_size)(\n",
    "            categorical_encoders[col](categorical_inputs[col])\n",
    "        )) for col in categorical_features\n",
    "    ]\n",
    "\n",
    "    all_features = Concatenate()(processed_continuous + processed_discrete + embedded_features)\n",
    "    return continuous_inputs, discrete_inputs, categorical_inputs, all_features\n",
    "\n",
    "# Build full model\n",
    "continuous_inputs, discrete_inputs, categorical_inputs, processed_features = build_preprocessing_model()\n",
    "x = Dense(128, activation='relu', kernel_initializer='he_normal')(processed_features)\n",
    "x = tf.keras.layers.Dropout(0.1)(x)\n",
    "x = Dense(64, activation='relu', kernel_initializer='he_normal')(x)\n",
    "x = tf.keras.layers.Dropout(0.1)(x)\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model_inputs = list(continuous_inputs.values()) + list(discrete_inputs.values()) + list(categorical_inputs.values())\n",
    "model = Model(inputs=model_inputs, outputs=output)\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "#model.summary()\n",
    "\n",
    "# Preprocess datasets\n",
    "def preprocess_batch(features, labels):\n",
    "    inputs = {\n",
    "        **{f\"{col}_input\": features[col] for col in continuous_features},\n",
    "        **{f\"{col}_input\": tf.cast(features[col], tf.int32) for col in discrete_features},\n",
    "        **{f\"{col}_input\": features[col] for col in categorical_features}\n",
    "    }\n",
    "    return inputs, labels\n",
    "\n",
    "train_dataset = train_dataset_raw.map(preprocess_batch).cache()\n",
    "val_dataset = val_dataset_raw.map(preprocess_batch).cache()\n",
    "test_dataset = test_dataset_raw.map(preprocess_batch).cache()\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [EarlyStopping(patience=15,\n",
    "                           restore_best_weights=True,\n",
    "                           monitor='val_loss'),\n",
    "            ReduceLROnPlateau(monitor='val_loss',\n",
    "                              fact=0.5,\n",
    "                              patience=15,\n",
    "                              min_lr=1e-6),\n",
    "            ModelCheckpoint('best_logistic_credit_model_tf.keras',\n",
    "                            monitor='val_loss',\n",
    "                            save_best_only=True)\n",
    "            ]\n",
    "\n",
    "# Train\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=50,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "print(f\"Test loss: {test_loss} - Test accuracy: {test_acc}\")\n",
    "\n",
    "# Save model\n",
    "custom_objects = {\n",
    "    'log1p_with_shape': log1p_with_shape,\n",
    "    'cast_to_float_with_shape': cast_to_float_with_shape\n",
    "}\n",
    "\n",
    "model.save(\"logistic_credit_model_tf.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Functional name=functional_2, built=True>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.models.load_model(\"credit_model_tf_large.keras\",\n",
    "                           custom_objects=custom_objects,\n",
    "                           safe_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For large dataset\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import fetch_openml\n",
    "from tensorflow.keras.layers import Dense, Input, Embedding, Flatten, Concatenate, BatchNormalization, IntegerLookup, StringLookup\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Placeholder: Replace with your huge dataset loading logic\n",
    "# For testing, we'll use credit-g; for production, use file-based loading below\n",
    "credit_data = fetch_openml(name='credit-g', version=1, as_frame=True)\n",
    "X = credit_data.data\n",
    "y = credit_data.target.map({'good': 1, 'bad': 0}).values\n",
    "\n",
    "# Define feature columns\n",
    "discrete_num = ['installment_commitment', 'residence_since', 'num_dependents', 'existing_credits']\n",
    "categorical_cols = X.select_dtypes(exclude='number').columns.tolist()\n",
    "con_num_features = ['duration', 'credit_amount']\n",
    "\n",
    "# Scalable dataset loading for huge datasets (uncomment and customize for your files)\n",
    "\"\"\"\n",
    "def create_tf_datasets(file_pattern, total_samples, train_size=0.8, val_size=0.1, batch_size=128, seed=42):\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        file_pattern=file_pattern,\n",
    "        batch_size=batch_size,\n",
    "        column_names=X.columns.tolist() + ['label'],  # Replace with your column names\n",
    "        label_name='label',\n",
    "        shuffle=True,\n",
    "        shuffle_seed=seed,\n",
    "        num_epochs=1\n",
    "    )\n",
    "    train_size = int(train_size * total_samples)\n",
    "    val_size = int(val_size * total_samples)\n",
    "    test_size = total_samples - train_size - val_size\n",
    "    train_dataset = dataset.take(train_size // batch_size)\n",
    "    val_dataset = dataset.skip(train_size // batch_size).take(val_size // batch_size)\n",
    "    test_dataset = dataset.skip((train_size + val_size) // batch_size)\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "# Example usage for huge dataset\n",
    "total_samples = 1000000  # Replace with your dataset size\n",
    "train_dataset_raw, val_dataset_raw, test_dataset_raw = create_tf_datasets(\n",
    "    file_pattern=\"path/to/files/*.csv\", total_samples=total_samples\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# For credit-g testing\n",
    "def create_tf_datasets(X, y, train_size=0.8, val_size=0.1, batch_size=128, seed=2025):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(X), y))\n",
    "    dataset = dataset.shuffle(buffer_size=len(X), seed=seed)  # Seed for consistency\n",
    "    n = len(X)\n",
    "    train_size = int(n * train_size)\n",
    "    val_size = int(n * val_size)\n",
    "    train_dataset = dataset.take(train_size)\n",
    "    val_dataset = dataset.skip(train_size).take(val_size)\n",
    "    test_dataset = dataset.skip(train_size + val_size)\n",
    "    return (\n",
    "        train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE),\n",
    "        val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE),\n",
    "        test_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "train_dataset_raw, val_dataset_raw, test_dataset_raw = create_tf_datasets(X, y)\n",
    "\n",
    "# Adapt lookup layers for huge datasets (limit to a subset)\n",
    "def adapt_preprocessing_layers(dataset, max_samples=10000):\n",
    "    ordinal_encoders = {col: IntegerLookup(output_mode='int', num_oov_indices=1) for col in discrete_num}\n",
    "    categorical_encoders = {col: StringLookup(output_mode='int', num_oov_indices=1) for col in categorical_cols}\n",
    "    \n",
    "    # Take a subset for adaptation (e.g., ~10k samples)\n",
    "    sample_dataset = dataset.take(max_samples // 128)  # Adjust based on batch size\n",
    "    for batch in sample_dataset:\n",
    "        features, _ = batch\n",
    "        for col in discrete_num:\n",
    "            ordinal_encoders[col].adapt(features[col])\n",
    "        for col in categorical_cols:\n",
    "            categorical_encoders[col].adapt(features[col])\n",
    "    \n",
    "    return ordinal_encoders, categorical_encoders\n",
    "\n",
    "ordinal_encoders, categorical_encoders = adapt_preprocessing_layers(train_dataset_raw)\n",
    "\n",
    "# Build model with preprocessing\n",
    "def build_preprocessing_model():\n",
    "    continuous_inputs = {col: Input(shape=(1,), dtype=tf.float32, name=f\"{col}_input\") for col in con_num_features}\n",
    "    discrete_inputs = {col: Input(shape=(1,), dtype=tf.int32, name=f\"{col}_input\") for col in discrete_num}\n",
    "    categorical_inputs = {col: Input(shape=(1,), dtype=tf.string, name=f\"{col}_input\") for col in categorical_cols}\n",
    "\n",
    "    processed_continuous = [\n",
    "        BatchNormalization(momentum=0.1, epsilon=1e-5)(\n",
    "            tf.keras.layers.Lambda(lambda x: tf.math.log1p(x))(continuous_inputs[col])\n",
    "        ) for col in con_num_features\n",
    "    ]\n",
    "\n",
    "    processed_discrete = [\n",
    "        tf.keras.layers.Lambda(lambda x: tf.cast(ordinal_encoders[col](x), tf.float32))(discrete_inputs[col])\n",
    "        for col in discrete_num\n",
    "    ]\n",
    "\n",
    "    embedding_size = 8\n",
    "    embedded_features = [\n",
    "        Flatten()(Embedding(input_dim=categorical_encoders[col].vocabulary_size(), output_dim=embedding_size)(\n",
    "            categorical_encoders[col](categorical_inputs[col])\n",
    "        )) for col in categorical_cols\n",
    "    ]\n",
    "\n",
    "    all_features = Concatenate()(processed_continuous + processed_discrete + embedded_features)\n",
    "    return continuous_inputs, discrete_inputs, categorical_inputs, all_features\n",
    "\n",
    "# Build full model\n",
    "continuous_inputs, discrete_inputs, categorical_inputs, processed_features = build_preprocessing_model()\n",
    "x = Dense(128, activation='relu')(processed_features)\n",
    "x = tf.keras.layers.Dropout(0.1)(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.1)(x)\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model_inputs = list(continuous_inputs.values()) + list(discrete_inputs.values()) + list(categorical_inputs.values())\n",
    "model = Model(inputs=model_inputs, outputs=output)\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Preprocess datasets\n",
    "def preprocess_batch(features, labels):\n",
    "    inputs = {\n",
    "        **{f\"{col}_input\": features[col] for col in con_num_features},\n",
    "        **{f\"{col}_input\": tf.cast(features[col], tf.int32) for col in discrete_num},\n",
    "        **{f\"{col}_input\": features[col] for col in categorical_cols}\n",
    "    }\n",
    "    return inputs, labels\n",
    "\n",
    "# No caching for huge datasets (enable for small datasets like credit-g)\n",
    "train_dataset = train_dataset_raw.map(preprocess_batch)  # .cache() optional for small data\n",
    "val_dataset = val_dataset_raw.map(preprocess_batch)      # .cache() optional\n",
    "test_dataset = test_dataset_raw.map(preprocess_batch)    # .cache() optional\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [EarlyStopping(patience=10, restore_best_weights=True)]\n",
    "\n",
    "# Train\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=50,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "print(f\"Test accuracy: {test_acc}\")\n",
    "\n",
    "# Save model\n",
    "model.save(\"credit_model_tf_large.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "def set_seeds(seed=2025):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seeds(2025)  # Using the same seed as TensorFlow model\n",
    "\n",
    "# Load dataset\n",
    "credit_data = fetch_openml(name='credit-g', version=1, as_frame=True)\n",
    "X = credit_data.data\n",
    "y = credit_data.target.map({'good': 1, 'bad': 0}).values\n",
    "\n",
    "# Define feature columns (same as in TensorFlow)\n",
    "discrete_num = ['installment_commitment', 'residence_since', 'num_dependents', 'existing_credits']\n",
    "categorical_cols = X.select_dtypes(exclude='number').columns.tolist()\n",
    "con_num_features = ['duration', 'credit_amount']\n",
    "\n",
    "# Split data using same proportions and seed as TensorFlow\n",
    "def manual_split(X, y, train_size=0.8, val_size=0.1, seed=2025):\n",
    "    np.random.seed(seed)\n",
    "    n = len(X)\n",
    "    train_idx = int(n * train_size)\n",
    "    val_idx = int(n * (train_size + val_size))\n",
    "    indices = np.random.permutation(n)\n",
    "    train_indices = indices[:train_idx]\n",
    "    val_indices = indices[train_idx:val_idx]\n",
    "    test_indices = indices[val_idx:]\n",
    "    return (\n",
    "        X.iloc[train_indices], X.iloc[val_indices], X.iloc[test_indices],\n",
    "        y[train_indices], y[val_indices], y[test_indices]\n",
    "    )\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = manual_split(X, y)\n",
    "\n",
    "# Calculate mean and std for continuous features (log-transformed)\n",
    "continuous_stats = {}\n",
    "for col in con_num_features:\n",
    "    log_values = np.log1p(X_train[col].values)\n",
    "    continuous_stats[col] = {'mean': log_values.mean(), 'std': log_values.std()}\n",
    "\n",
    "# Custom Dataset Class\n",
    "class CreditDataset(Dataset):\n",
    "    def __init__(self, X, y, categorical_cols, discrete_num, con_num_features):\n",
    "        self.X = X.reset_index(drop=True)\n",
    "        self.y = y\n",
    "        self.categorical_cols = categorical_cols\n",
    "        self.discrete_num = discrete_num\n",
    "        self.con_num_features = con_num_features\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features = {}\n",
    "        for col in self.con_num_features:\n",
    "            features[col] = torch.tensor(self.X.loc[idx, col], dtype=torch.float32).unsqueeze(0)\n",
    "        \n",
    "        for col in self.discrete_num:\n",
    "            features[col] = torch.tensor(self.X.loc[idx, col], dtype=torch.int64).unsqueeze(0)\n",
    "        \n",
    "        for col in self.categorical_cols:\n",
    "            features[col] = self.X.loc[idx, col]\n",
    "        \n",
    "        label = torch.tensor(self.y[idx], dtype=torch.float32)\n",
    "        return features, label\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CreditDataset(X_train, y_train, categorical_cols, discrete_num, con_num_features)\n",
    "val_dataset = CreditDataset(X_val, y_val, categorical_cols, discrete_num, con_num_features)\n",
    "test_dataset = CreditDataset(X_test, y_test, categorical_cols, discrete_num, con_num_features)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 128  # Same as TensorFlow\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Preprocessing Layers - aligning with TensorFlow implementation\n",
    "class Preprocessor(nn.Module):\n",
    "    def __init__(self, con_num_features, discrete_num, categorical_cols, train_dataset, continuous_stats):\n",
    "        super(Preprocessor, self).__init__()\n",
    "        self.con_num_features = con_num_features\n",
    "        self.discrete_num = discrete_num\n",
    "        self.categorical_cols = categorical_cols\n",
    "        self.continuous_stats = continuous_stats\n",
    "\n",
    "        # Discrete numerical features: Integer encoding (like IntegerLookup in TF)\n",
    "        self.ordinal_encoders = {}\n",
    "        for col in discrete_num:\n",
    "            unique_values = sorted(train_dataset.X[col].unique())\n",
    "            self.ordinal_encoders[col] = {val: idx + 1 for idx, val in enumerate(unique_values)}\n",
    "            # +1 for 1-based indexing (0 reserved for OOV)\n",
    "        \n",
    "        # Categorical features: String lookup (like StringLookup in TF)\n",
    "        self.string_lookups = {}\n",
    "        self.embeddings = nn.ModuleDict()\n",
    "        \n",
    "        for col in categorical_cols:\n",
    "            unique_values = sorted(train_dataset.X[col].unique())\n",
    "            self.string_lookups[col] = {val: idx + 1 for idx, val in enumerate(unique_values)}\n",
    "            # +1 for 1-based indexing (0 reserved for OOV)\n",
    "            \n",
    "            # Use embedding size of 8 to match TensorFlow\n",
    "            self.embeddings[col] = nn.Embedding(\n",
    "                num_embeddings=len(unique_values) + 1,  # +1 for OOV/padding\n",
    "                embedding_dim=8  # Same as TensorFlow\n",
    "            )\n",
    "\n",
    "        # Batch normalization layers for continuous features\n",
    "        self.batch_norms = nn.ModuleDict({\n",
    "            col: nn.BatchNorm1d(1, momentum=0.1, eps=1e-5)  # Match TF params\n",
    "            for col in con_num_features\n",
    "        })\n",
    "\n",
    "    def forward(self, features):\n",
    "        processed_features = []\n",
    "\n",
    "        # Continuous features: Log transform + BatchNorm (like TensorFlow)\n",
    "        for col in self.con_num_features:\n",
    "            # Log1p transform\n",
    "            log_transformed = torch.log1p(features[col])\n",
    "            # Apply batch normalization\n",
    "            normalized = self.batch_norms[col](log_transformed)\n",
    "            processed_features.append(normalized)\n",
    "\n",
    "        # Discrete numerical features\n",
    "        for col in self.discrete_num:\n",
    "            # Map to integers with handling for OOV values\n",
    "            indices = torch.tensor([\n",
    "                self.ordinal_encoders[col].get(int(f.item()), 0) for f in features[col]\n",
    "            ], dtype=torch.long, device=features[col].device)\n",
    "            # Convert to float to match TensorFlow behavior\n",
    "            processed_features.append(indices.unsqueeze(1).float())\n",
    "\n",
    "        # Categorical features: lookup + embeddings\n",
    "        for col in self.categorical_cols:\n",
    "            # Get indices with OOV handling\n",
    "            indices = torch.tensor([\n",
    "                self.string_lookups[col].get(str(f), 0) for f in features[col]\n",
    "            ], dtype=torch.long, device=next(self.embeddings[col].parameters()).device)\n",
    "            \n",
    "            # Apply embedding\n",
    "            embedded = self.embeddings[col](indices)\n",
    "            processed_features.append(embedded.view(embedded.size(0), -1))\n",
    "\n",
    "        # Concatenate all features\n",
    "        return torch.cat(processed_features, dim=1)\n",
    "\n",
    "# Calculate input dimension\n",
    "def calc_input_dim(con_num_features, discrete_num, categorical_cols, embedding_size=8):\n",
    "    return (\n",
    "        len(con_num_features) +  # Continuous features\n",
    "        len(discrete_num) +      # Discrete features\n",
    "        len(categorical_cols) * embedding_size  # Embedded categorical features\n",
    "    )\n",
    "\n",
    "# Build model to match TensorFlow architecture\n",
    "class CreditModel(nn.Module):\n",
    "    def __init__(self, preprocessor, input_dim):\n",
    "        super(CreditModel, self).__init__()\n",
    "        self.preprocessor = preprocessor\n",
    "        \n",
    "        # Match TensorFlow architecture\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.dropout1 = nn.Dropout(0.1)  # Match TF dropout rate\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "        self.output_layer = nn.Linear(64, 1)\n",
    "        \n",
    "        # Initialize weights (helps match TF performance)\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self, features):\n",
    "        x = self.preprocessor(features)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        output = torch.sigmoid(self.output_layer(x))\n",
    "        return output\n",
    "\n",
    "# Initialize preprocessor and model\n",
    "input_dim = calc_input_dim(con_num_features, discrete_num, categorical_cols)\n",
    "preprocessor = Preprocessor(con_num_features, discrete_num, categorical_cols, train_dataset, continuous_stats)\n",
    "model = CreditModel(preprocessor, input_dim)\n",
    "\n",
    "# Loss and optimizer (match TensorFlow params)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Match TF learning rate\n",
    "\n",
    "# Early stopping implementation (similar to TF EarlyStopping)\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, delta=0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_score = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.best_model_state = None\n",
    "    \n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "        \n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.best_model_state = model.state_dict().copy()\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.best_model_state = model.state_dict().copy()\n",
    "            self.counter = 0\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, num_epochs=50):\n",
    "    early_stopping = EarlyStopping(patience=10)\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for features, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(features).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = correct / total\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for features, labels in val_loader:\n",
    "                outputs = model(features).squeeze()\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                predicted = (outputs > 0.5).float()\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_acc = correct / total\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, '\n",
    "              f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, '\n",
    "              f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "        \n",
    "        # Early stopping\n",
    "        early_stopping(val_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            model.load_state_dict(early_stopping.best_model_state)\n",
    "            break\n",
    "    \n",
    "    if not early_stopping.early_stop and early_stopping.best_model_state is not None:\n",
    "        model.load_state_dict(early_stopping.best_model_state)\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "# Train the model\n",
    "train_losses, val_losses = train_model(model, train_loader, val_loader)\n",
    "\n",
    "# Evaluate on test set\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for features, labels in test_loader:\n",
    "        outputs = model(features).squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "test_accuracy = correct / total\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), \"credit_model_pytorch.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
